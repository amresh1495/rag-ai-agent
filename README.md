# Customer Support RAG Chatbot

## Objective

A Retrieval-Augmented Generation (RAG) chatbot trained on customer support documentation (provided as PDF and DOCX files) to assist users by answering their queries and providing relevant support information extracted directly from these documents.

## Features

*   Answers questions based on the content of the provided PDF and DOCX documents.
*   Responds with "I Don't know" if the answer cannot be confidently found within the documents, based on a relevance score threshold.
*   Provides a user-friendly web interface built with Streamlit.
*   Displays snippets from the source documents that were used to generate the answer, enhancing transparency.
*   Persists the knowledge base (vector store) locally for faster startup after the initial setup.

## Project Structure

The project is organized as follows:

*   `app.py`: The main Streamlit web application file that provides the user interface for the chatbot.
*   `rag_core.py`: Contains the core logic for the RAG pipeline, including data loading, document splitting, embedding generation, vector store management (FAISS), and the query-answering mechanism with the "I Don't Know" feature.
*   `converter.py`: A utility script responsible for converting the original PDF and DOCX documents into plain text (`.txt`) files. This is a preliminary step to prepare the documents for the RAG pipeline. The chatbot application (`app.py`) uses the `.txt` files generated by this script (or if not present, `rag_core.py` directly loads `.txt` files).
*   `requirements.txt`: Lists all Python dependencies required to run the project.
*   `faiss_index/`: This directory is automatically created when the application is first run. It stores the FAISS vector index, which is the knowledge base of the chatbot, allowing for quicker startups on subsequent uses.
*   **Document Files**:
    *   Original source documents (`.pdf`, `.docx` files) are located in the root directory. These form the knowledge base.
    *   Text versions (`.txt` files) of these documents are generated by `converter.py` and used by `rag_core.py`.

## Setup Instructions

### Prerequisites

*   Python 3.8 or higher.
*   `pip` (Python package installer).

### Environment Setup (Recommended)

It is highly recommended to use a virtual environment to manage project dependencies and avoid conflicts with system-wide packages.

1.  **Create a virtual environment:**
    Open your terminal or command prompt in the project's root directory and run:
    ```bash
    python -m venv .venv
    ```

2.  **Activate the virtual environment:**
    *   On Windows:
        ```bash
        .\.venv\Scripts\activate
        ```
    *   On macOS and Linux:
        ```bash
        source .venv/bin/activate
        ```
    You should see the name of your virtual environment (e.g., `(.venv)`) in your terminal prompt.

### Install Dependencies

Once your virtual environment is activated, install the required Python packages:

```bash
pip install -r requirements.txt
```

## Running the Application

1.  **Ensure Documents are Present:**
    Make sure your source `.pdf` and `.docx` documents are in the root directory of the project. If you haven't already, run `converter.py` once to generate the `.txt` files that `rag_core.py` will use:
    ```bash
    python converter.py
    ```
    (The Streamlit app `app.py` relies on `rag_core.py` which expects `.txt` files. The initialization logic in `app.py` will handle creating the vector store from these `.txt` files.)

2.  **Start the Streamlit Application:**
    In your terminal (with the virtual environment activated), run the following command:
    ```bash
    streamlit run app.py
    ```

3.  **Access the Chatbot:**
    Streamlit will typically open the application in your default web browser automatically. If not, it will display a local URL (e.g., `http://localhost:8501`) that you can navigate to.

    **Note:** The first time you run the application, it might take a few minutes to initialize. This is because it needs to:
    *   Load the documents.
    *   Split them into manageable chunks.
    *   Generate embeddings for these chunks using a sentence transformer model (which might be downloaded if not cached).
    *   Create and save the FAISS vector store.
    Subsequent runs will be much faster as the vector store (`faiss_index/`) will be loaded from disk. The language model (e.g., 'google/flan-t5-small') might also be downloaded by Hugging Face Transformers on its first use.

## How it Works (Briefly)

The chatbot employs a Retrieval-Augmented Generation (RAG) architecture:

1.  **Document Conversion (Preliminary Step):** Original PDF and DOCX documents are converted into plain text files using `converter.py`.
2.  **Data Loading and Chunking:** The text documents are loaded and split into smaller, overlapping chunks. This makes it easier for the model to find specific pieces of information.
3.  **Embedding Generation:** Each text chunk is converted into a numerical representation (embedding) using a sentence transformer model (`sentence-transformers/all-MiniLM-L6-v2`). These embeddings capture the semantic meaning of the text.
4.  **Vector Storage:** The embeddings are stored in a FAISS (Facebook AI Similarity Search) vector store. This allows for efficient searching of text chunks that are semantically similar to a user's query.
5.  **Retrieval:** When a user asks a question:
    *   The query is embedded using the same sentence transformer.
    *   The FAISS vector store is searched to find the text chunks with embeddings most similar (closest in L2 distance) to the query's embedding.
    *   A relevance threshold is applied to these retrieved chunks. If the relevance scores (based on L2 distance) do not meet the threshold, the chatbot responds with "I Don't know."
6.  **Answer Generation:** If relevant chunks are found:
    *   These chunks, along with the original query, are passed to a pre-trained language model (`google/flan-t5-small`).
    *   The language model generates a natural language answer based on the information contained in the retrieved chunks and the user's query.

## Data Files

The chatbot's knowledge base is derived from the `.pdf` and `.docx` files located in the root directory of this project. These files are processed as described above to enable the RAG pipeline. Currently, these include various "Summary of Benefits" (SOB) documents and a medical questions document.
