# Customer Support RAG Chatbot

## Objective

A Retrieval-Augmented Generation (RAG) chatbot trained on customer support documentation (provided as PDF and DOCX files) to assist users by answering their queries and providing relevant support information extracted directly from these documents.

## Features

*   Answers questions based on the content of the provided PDF and DOCX documents.
*   Responds with "I Don't know" if the answer cannot be confidently found within the documents, based on a relevance score threshold.
*   Provides a user-friendly web interface built with Streamlit.
*   Displays snippets from the source documents that were used to generate the answer, enhancing transparency.
*   Persists the knowledge base (vector store) locally for faster startup after the initial setup.

## Project Architecture

This project follows a modular architecture designed for a Retrieval-Augmented Generation (RAG) system, with distinct components handling different aspects of the chatbot's functionality:

*   **Data Files (Knowledge Base):**
    *   **Original Documents:** These are the `.pdf` and `.docx` files provided by the user/customer (e.g., Summary of Benefits, medical guidelines). They constitute the raw knowledge source.
    *   **Text Documents (`.txt`):** Plain text versions of the original documents, generated by `converter.py`. These are directly processed by the RAG pipeline.

*   **`converter.py` (Document Preprocessing):**
    *   A utility script responsible for converting the original `.pdf` and `.docx` documents into `.txt` files.
    *   This is typically run as an initial, one-time step to prepare the data for ingestion into the RAG system.

*   **`rag_core.py` (Core RAG Engine):**
    *   This module is the heart of the chatbot's intelligence. Its responsibilities include:
        *   **Data Loading:** Loading the `.txt` documents.
        *   **Document Chunking:** Splitting the loaded documents into smaller, manageable, and often overlapping chunks to ensure effective retrieval.
        *   **Embedding Generation:** Using a Sentence Transformer model (e.g., `sentence-transformers/all-MiniLM-L6-v2`) to convert text chunks into dense vector embeddings that capture semantic meaning.
        *   **Vector Store Management (FAISS):**
            *   Creating a FAISS (Facebook AI Similarity Search) index from the generated embeddings.
            *   Saving this index to the `faiss_index/` directory for persistence.
            *   Loading the existing FAISS index on subsequent runs to speed up initialization.
        *   **Language Model (LLM) Initialization:** Loading and initializing a generative language model (e.g., `google/flan-t5-small` via Hugging Face Transformers) responsible for generating human-like answers.
        *   **Query Answering (`answer_query` function):**
            *   Embedding the user's query using the same Sentence Transformer model.
            *   Retrieving the most relevant document chunks from the FAISS index by comparing similarity (L2 distance) between the query embedding and the stored document chunk embeddings.
            *   Implementing the "I Don't Know" (IDK) logic: If the retrieved documents do not meet a predefined relevance threshold (based on L2 distance), the system decides it cannot confidently answer.
            *   If relevant documents are found, they are combined with the user's query to form a prompt for the LLM.
            *   The LLM generates a textual answer based on the provided context (retrieved chunks) and the query.

*   **`faiss_index/` (Persistent Knowledge Base):**
    *   This directory stores the FAISS vector index. By saving the index, the system avoids re-computing embeddings for all documents every time it starts, significantly reducing startup time after the initial setup.

*   **`app.py` (User Interface - Streamlit):**
    *   Provides a web-based, interactive chat interface for users.
    *   Captures user queries.
    *   Calls the `initialize_rag_components` function (which in turn uses `rag_core.py` functions) to set up the RAG pipeline components, utilizing Streamlit's caching and session state to manage these resources efficiently.
    *   Invokes the `answer_query` function from `rag_core.py` to process the query and get a response.
    *   Displays the chatbot's generated answer and the source document snippets (with their relevance scores) that were used or considered.

### Data Flow (Typical Query)

1.  **User Input:** The user types a question into the Streamlit web interface (`app.py`).
2.  **Query Processing Request:** `app.py` passes the query to the `answer_query` function within `rag_core.py`.
3.  **Information Retrieval:**
    *   `rag_core.py` embeds the user's query.
    *   It searches the `faiss_index/` using the query embedding to find the most relevant document chunks.
4.  **Relevance Assessment (IDK Logic):**
    *   The relevance of retrieved chunks is assessed against a pre-defined L2 distance threshold.
    *   If no chunks are found or if the top-retrieved chunks are not relevant enough, an "I Don't know" response is formulated.
5.  **Contextual Prompting for LLM:** If relevant chunks are identified, they are formatted along with the user's query into a prompt for the language model.
6.  **Answer Generation:** The language model (e.g., Flan-T5-small) processes the prompt and generates a textual answer.
7.  **Display to User:** The generated answer, along with any source document snippets and debug information (like scores), is sent back to `app.py` and displayed in the chat interface.

## Project Structure

The project is organized as follows:

*   `app.py`: The main Streamlit web application file that provides the user interface for the chatbot.
*   `rag_core.py`: Contains the core logic for the RAG pipeline, including data loading, document splitting, embedding generation, vector store management (FAISS), and the query-answering mechanism with the "I Don't Know" feature.
*   `converter.py`: A utility script responsible for converting the original PDF and DOCX documents into plain text (`.txt`) files. This is a preliminary step to prepare the documents for the RAG pipeline. The chatbot application (`app.py`) uses the `.txt` files generated by this script (or if not present, `rag_core.py` directly loads `.txt` files).
*   `requirements.txt`: Lists all Python dependencies required to run the project.
*   `faiss_index/`: This directory is automatically created when the application is first run. It stores the FAISS vector index, which is the knowledge base of the chatbot, allowing for quicker startups on subsequent uses.
*   **Document Files**:
    *   Original source documents (`.pdf`, `.docx` files) are located in the root directory. These form the knowledge base.
    *   Text versions (`.txt` files) of these documents are generated by `converter.py` and used by `rag_core.py`.

## Setup Instructions

### Prerequisites

*   Python 3.8 or higher.
*   `pip` (Python package installer).

### Environment Setup (Recommended)

It is highly recommended to use a virtual environment to manage project dependencies and avoid conflicts with system-wide packages.

1.  **Create a virtual environment:**
    Open your terminal or command prompt in the project's root directory and run:
    ```bash
    python -m venv .venv
    ```

2.  **Activate the virtual environment:**
    *   On Windows:
        ```bash
        .\.venv\Scripts\activate
        ```
    *   On macOS and Linux:
        ```bash
        source .venv/bin/activate
        ```
    You should see the name of your virtual environment (e.g., `(.venv)`) in your terminal prompt.

### Install Dependencies

Once your virtual environment is activated, install the required Python packages:

```bash
pip install -r requirements.txt
```

## Running the Application

1.  **Ensure Documents are Present:**
    Make sure your source `.pdf` and `.docx` documents are in the root directory of the project. If you haven't already, run `converter.py` once to generate the `.txt` files that `rag_core.py` will use:
    ```bash
    python converter.py
    ```
    (The Streamlit app `app.py` relies on `rag_core.py` which expects `.txt` files. The initialization logic in `app.py` will handle creating the vector store from these `.txt` files.)

2.  **Start the Streamlit Application:**
    In your terminal (with the virtual environment activated), run the following command:
    ```bash
    streamlit run app.py
    ```

3.  **Access the Chatbot:**
    Streamlit will typically open the application in your default web browser automatically. If not, it will display a local URL (e.g., `http://localhost:8501`) that you can navigate to.

    **Note:** The first time you run the application, it might take a few minutes to initialize. This is because it needs to:
    *   Load the documents.
    *   Split them into manageable chunks.
    *   Generate embeddings for these chunks using a sentence transformer model (which might be downloaded if not cached).
    *   Create and save the FAISS vector store.
    Subsequent runs will be much faster as the vector store (`faiss_index/`) will be loaded from disk. The language model (e.g., 'google/flan-t5-small') might also be downloaded by Hugging Face Transformers on its first use.

## How it Works (Briefly)

The chatbot employs a Retrieval-Augmented Generation (RAG) architecture:

1.  **Document Conversion (Preliminary Step):** Original PDF and DOCX documents are converted into plain text files using `converter.py`.
2.  **Data Loading and Chunking:** The text documents are loaded and split into smaller, overlapping chunks. This makes it easier for the model to find specific pieces of information.
3.  **Embedding Generation:** Each text chunk is converted into a numerical representation (embedding) using a sentence transformer model (`sentence-transformers/all-MiniLM-L6-v2`). These embeddings capture the semantic meaning of the text.
4.  **Vector Storage:** The embeddings are stored in a FAISS (Facebook AI Similarity Search) vector store. This allows for efficient searching of text chunks that are semantically similar to a user's query.
5.  **Retrieval:** When a user asks a question:
    *   The query is embedded using the same sentence transformer.
    *   The FAISS vector store is searched to find the text chunks with embeddings most similar (closest in L2 distance) to the query's embedding.
    *   A relevance threshold is applied to these retrieved chunks. If the relevance scores (based on L2 distance) do not meet the threshold, the chatbot responds with "I Don't know."
6.  **Answer Generation:** If relevant chunks are found:
    *   These chunks, along with the original query, are passed to a pre-trained language model (`google/flan-t5-small`).
    *   The language model generates a natural language answer based on the information contained in the retrieved chunks and the user's query.

## Data Files

The chatbot's knowledge base is derived from the `.pdf` and `.docx` files located in the root directory of this project. These files are processed as described above to enable the RAG pipeline. Currently, these include various "Summary of Benefits" (SOB) documents and a medical questions document.
